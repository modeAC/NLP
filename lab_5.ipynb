{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"92ec957ebad9445b83083da1b7f2f620":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5665872fa18043cd95d3b5ec42a62684","IPY_MODEL_29eaa13707c5489ebc874640f2d317f9","IPY_MODEL_4fda2d263df84838806a2cd009606ef0"],"layout":"IPY_MODEL_652923b238bb479ca5c8d0f7913ff18b"}},"5665872fa18043cd95d3b5ec42a62684":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_08ae28512da24f07acf8f1342bba3bf2","placeholder":"​","style":"IPY_MODEL_bba8757c73d641c89deaf2569055a7cc","value":"Loading checkpoint shards: 100%"}},"29eaa13707c5489ebc874640f2d317f9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c40c8fc8fd234bfb98482631c8090e04","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f645e160bb56433fa78bde14acc4be6b","value":2}},"4fda2d263df84838806a2cd009606ef0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a807238e0fdd43748125ea980f7cc9d3","placeholder":"​","style":"IPY_MODEL_91cf7db385134d1bb8dd0e852eabb075","value":" 2/2 [01:10&lt;00:00, 32.19s/it]"}},"652923b238bb479ca5c8d0f7913ff18b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08ae28512da24f07acf8f1342bba3bf2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bba8757c73d641c89deaf2569055a7cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c40c8fc8fd234bfb98482631c8090e04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f645e160bb56433fa78bde14acc4be6b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a807238e0fdd43748125ea980f7cc9d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91cf7db385134d1bb8dd0e852eabb075":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"GLXwJqbjtPho","executionInfo":{"status":"ok","timestamp":1717539064179,"user_tz":-180,"elapsed":8664,"user":{"displayName":"Nazarii Tovstenko","userId":"04428992633330999802"}}},"outputs":[],"source":["!pip install -q accelerate==0.21.0 bitsandbytes==0.40.2 transformers==4.31.0 pandas datasets tqdm chromadb sentence-transformers"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"vC6-ujg7FItL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717539066881,"user_tz":-180,"elapsed":2713,"user":{"displayName":"Nazarii Tovstenko","userId":"04428992633330999802"}},"outputId":"c87ca001-911d-4027-aa16-6257ee1b1905"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import os\n","import re\n","from collections import defaultdict\n","\n","import torch\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","\n","import pandas as pd\n","import datasets\n","import chromadb\n","from tqdm import tqdm\n","from sentence_transformers import SentenceTransformer"],"metadata":{"id":"nAMzy_0FtaUZ","executionInfo":{"status":"ok","timestamp":1717539078279,"user_tz":-180,"elapsed":11423,"user":{"displayName":"Nazarii Tovstenko","userId":"04428992633330999802"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["dataset = datasets.load_dataset(\"jamescalam/llama-2-arxiv-papers-chunked\")\n","df = dataset['train'].to_pandas()\n","df.head(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"g9_PiWoz6-Ay","outputId":"b3b999c9-9ea7-4696-afec-bf4222865ccf","executionInfo":{"status":"ok","timestamp":1717539086994,"user_tz":-180,"elapsed":8731,"user":{"displayName":"Nazarii Tovstenko","userId":"04428992633330999802"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["         doi chunk-id                                              chunk  \\\n","0  1102.0183        0  High-Performance Neural Networks\\nfor Visual O...   \n","1  1102.0183        1  January 2011\\nAbstract\\nWe present a fast, ful...   \n","2  1102.0183        2  promising architectures for such tasks. The mo...   \n","3  1102.0183        3  Mutch and Lowe, 2008), whose \n","lters are \n","xed, ...   \n","4  1102.0183        4  We evaluate various networks on the handwritte...   \n","\n","          id                                              title  \\\n","0  1102.0183  High-Performance Neural Networks for Visual Ob...   \n","1  1102.0183  High-Performance Neural Networks for Visual Ob...   \n","2  1102.0183  High-Performance Neural Networks for Visual Ob...   \n","3  1102.0183  High-Performance Neural Networks for Visual Ob...   \n","4  1102.0183  High-Performance Neural Networks for Visual Ob...   \n","\n","                                             summary  \\\n","0  We present a fast, fully parameterizable GPU i...   \n","1  We present a fast, fully parameterizable GPU i...   \n","2  We present a fast, fully parameterizable GPU i...   \n","3  We present a fast, fully parameterizable GPU i...   \n","4  We present a fast, fully parameterizable GPU i...   \n","\n","                           source  \\\n","0  http://arxiv.org/pdf/1102.0183   \n","1  http://arxiv.org/pdf/1102.0183   \n","2  http://arxiv.org/pdf/1102.0183   \n","3  http://arxiv.org/pdf/1102.0183   \n","4  http://arxiv.org/pdf/1102.0183   \n","\n","                                             authors      categories  \\\n","0  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n","1  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n","2  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n","3  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n","4  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n","\n","                         comment journal_ref primary_category published  \\\n","0  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n","1  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n","2  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n","3  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n","4  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n","\n","    updated references  \n","0  20110201         []  \n","1  20110201         []  \n","2  20110201         []  \n","3  20110201         []  \n","4  20110201         []  "],"text/html":["\n","  <div id=\"df-3b888e14-a01e-4608-bc24-4f06f99fb4aa\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>doi</th>\n","      <th>chunk-id</th>\n","      <th>chunk</th>\n","      <th>id</th>\n","      <th>title</th>\n","      <th>summary</th>\n","      <th>source</th>\n","      <th>authors</th>\n","      <th>categories</th>\n","      <th>comment</th>\n","      <th>journal_ref</th>\n","      <th>primary_category</th>\n","      <th>published</th>\n","      <th>updated</th>\n","      <th>references</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1102.0183</td>\n","      <td>0</td>\n","      <td>High-Performance Neural Networks\\nfor Visual O...</td>\n","      <td>1102.0183</td>\n","      <td>High-Performance Neural Networks for Visual Ob...</td>\n","      <td>We present a fast, fully parameterizable GPU i...</td>\n","      <td>http://arxiv.org/pdf/1102.0183</td>\n","      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n","      <td>[cs.AI, cs.NE]</td>\n","      <td>12 pages, 2 figures, 5 tables</td>\n","      <td>None</td>\n","      <td>cs.AI</td>\n","      <td>20110201</td>\n","      <td>20110201</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1102.0183</td>\n","      <td>1</td>\n","      <td>January 2011\\nAbstract\\nWe present a fast, ful...</td>\n","      <td>1102.0183</td>\n","      <td>High-Performance Neural Networks for Visual Ob...</td>\n","      <td>We present a fast, fully parameterizable GPU i...</td>\n","      <td>http://arxiv.org/pdf/1102.0183</td>\n","      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n","      <td>[cs.AI, cs.NE]</td>\n","      <td>12 pages, 2 figures, 5 tables</td>\n","      <td>None</td>\n","      <td>cs.AI</td>\n","      <td>20110201</td>\n","      <td>20110201</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1102.0183</td>\n","      <td>2</td>\n","      <td>promising architectures for such tasks. The mo...</td>\n","      <td>1102.0183</td>\n","      <td>High-Performance Neural Networks for Visual Ob...</td>\n","      <td>We present a fast, fully parameterizable GPU i...</td>\n","      <td>http://arxiv.org/pdf/1102.0183</td>\n","      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n","      <td>[cs.AI, cs.NE]</td>\n","      <td>12 pages, 2 figures, 5 tables</td>\n","      <td>None</td>\n","      <td>cs.AI</td>\n","      <td>20110201</td>\n","      <td>20110201</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1102.0183</td>\n","      <td>3</td>\n","      <td>Mutch and Lowe, 2008), whose \flters are \fxed, ...</td>\n","      <td>1102.0183</td>\n","      <td>High-Performance Neural Networks for Visual Ob...</td>\n","      <td>We present a fast, fully parameterizable GPU i...</td>\n","      <td>http://arxiv.org/pdf/1102.0183</td>\n","      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n","      <td>[cs.AI, cs.NE]</td>\n","      <td>12 pages, 2 figures, 5 tables</td>\n","      <td>None</td>\n","      <td>cs.AI</td>\n","      <td>20110201</td>\n","      <td>20110201</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1102.0183</td>\n","      <td>4</td>\n","      <td>We evaluate various networks on the handwritte...</td>\n","      <td>1102.0183</td>\n","      <td>High-Performance Neural Networks for Visual Ob...</td>\n","      <td>We present a fast, fully parameterizable GPU i...</td>\n","      <td>http://arxiv.org/pdf/1102.0183</td>\n","      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n","      <td>[cs.AI, cs.NE]</td>\n","      <td>12 pages, 2 figures, 5 tables</td>\n","      <td>None</td>\n","      <td>cs.AI</td>\n","      <td>20110201</td>\n","      <td>20110201</td>\n","      <td>[]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3b888e14-a01e-4608-bc24-4f06f99fb4aa')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-3b888e14-a01e-4608-bc24-4f06f99fb4aa button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-3b888e14-a01e-4608-bc24-4f06f99fb4aa');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-ef639d13-f57b-4ec2-a49b-352bef3a232b\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ef639d13-f57b-4ec2-a49b-352bef3a232b')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-ef639d13-f57b-4ec2-a49b-352bef3a232b button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 4838,\n  \"fields\": [\n    {\n      \"column\": \"doi\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 49,\n        \"samples\": [\n          \"1412.7062\",\n          \"2103.15691\",\n          \"2305.13245\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk-id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1029,\n        \"samples\": [\n          \"290\",\n          \"534\",\n          \"261\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3542,\n        \"samples\": [\n          \"2018), to classify and segment images and videos (Wang et al., 2018c; Hu et al., 2017) and 3D\\nmeshes and point clouds (Wang et al., 2018d), to classify regions in images (Chen et al., 2018a), to\\nperform semi-supervised text classi\\fcation (Kipf and Welling, 2017), and in machine translation\\n(Vaswani et al., 2017; Shaw et al., 2018; Gulcehre et al., 2018). They have been used within both\\nmodel-free (Wang et al., 2018b) and model-based (Hamrick et al., 2017; Pascanu et al., 2017;\\nSanchez-Gonzalez et al., 2018) continuous control, for model-free reinforcement learning (Hamrick\\net al., 2018; Zambaldi et al., 2018), and for more classical approaches to planning (Toyer et al.,\\n2017).\\nMany traditional computer science problems, which involve reasoning about discrete entities and\\nstructure, have also been explored with graph neural networks, such as combinatorial optimization\\n(Bello et al., 2016; Nowak et al., 2017; Dai et al., 2017), boolean satis\\fability (Selsam et al., 2018),\\nprogram representation and veri\\fcation (Allamanis et al., 2018; Li et al., 2016), modeling cellular\",\n          \"same \\ufb01xed length as the 224-network. As such, this\\n180-network has exactly the same parameters as the\\n224-network in each layer. In other words, during\\ntraining we implement the varying-input-size SPP-net\\nby two \\ufb01xed-size networks that share parameters.\\nTo reduce the overhead to switch from one network\\n(e.g., 224) to the other ( e.g., 180), we train each full\\nepoch on one network, and then switch to the other\\none (keeping all weights) for the next full epoch. This\\nis iterated. In experiments, we \\ufb01nd the convergence\\nrate of this multi-size training to be similar to the\\nabove single-size training.\\nThe main purpose of our multi-size training is to\\nsimulate the varying input sizes while still leveraging\\nthe existing well-optimized \\ufb01xed-size implementations. Besides the above two-scale implementation, we\\nhave also tested a variant using s\\u0002sas input where\\nsis randomly and uniformly sampled from [180;224]\\nat each epoch. We report the results of both variants\\nin the experiment section.\\nNote that the above single/multi-size solutions are\\nfor training only. At the testing stage, it is straightforward to apply SPP-net on images of any sizes.\\n5\",\n          \"the one-shot setting, 85.0 F1 in the few-shot setting. Similarly, GPT-3 achieves 64.3% accuracy on TriviaQA in the\\nzero-shot setting, 68.0% in the one-shot setting, and 71.2% in the few-shot setting, the last of which is state-of-the-art\\nrelative to \\ufb01ne-tuned models operating in the same closed-book setting.\\nGPT-3 also displays one-shot and few-shot pro\\ufb01ciency at tasks designed to test rapid adaption or on-the-\\ufb02y reasoning,\\nwhich include unscrambling words, performing arithmetic, and using novel words in a sentence after seeing them\\nde\\ufb01ned only once. We also show that in the few-shot setting, GPT-3 can generate synthetic news articles which human\\nevaluators have dif\\ufb01culty distinguishing from human-generated articles.\\nAt the same time, we also \\ufb01nd some tasks on which few-shot performance struggles, even at the scale of GPT-3. This\\nincludes natural language inference tasks like the ANLI dataset, and some reading comprehension datasets like RACE\\nor QuAC. By presenting a broad characterization of GPT-3\\u2019s strengths and weaknesses, including these limitations, we\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 49,\n        \"samples\": [\n          \"1412.7062\",\n          \"2103.15691\",\n          \"2305.13245\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 49,\n        \"samples\": [\n          \"Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs\",\n          \"ViViT: A Video Vision Transformer\",\n          \"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 49,\n        \"samples\": [\n          \"Deep Convolutional Neural Networks (DCNNs) have recently shown state of the\\nart performance in high level vision tasks, such as image classification and\\nobject detection. This work brings together methods from DCNNs and\\nprobabilistic graphical models for addressing the task of pixel-level\\nclassification (also called \\\"semantic image segmentation\\\"). We show that\\nresponses at the final layer of DCNNs are not sufficiently localized for\\naccurate object segmentation. This is due to the very invariance properties\\nthat make DCNNs good for high level tasks. We overcome this poor localization\\nproperty of deep networks by combining the responses at the final DCNN layer\\nwith a fully connected Conditional Random Field (CRF). Qualitatively, our\\n\\\"DeepLab\\\" system is able to localize segment boundaries at a level of accuracy\\nwhich is beyond previous methods. Quantitatively, our method sets the new\\nstate-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching\\n71.6% IOU accuracy in the test set. We show how these results can be obtained\\nefficiently: Careful network re-purposing and a novel application of the 'hole'\\nalgorithm from the wavelet community allow dense computation of neural net\\nresponses at 8 frames per second on a modern GPU.\",\n          \"We present pure-transformer based models for video classification, drawing\\nupon the recent success of such models in image classification. Our model\\nextracts spatio-temporal tokens from the input video, which are then encoded by\\na series of transformer layers. In order to handle the long sequences of tokens\\nencountered in video, we propose several, efficient variants of our model which\\nfactorise the spatial- and temporal-dimensions of the input. Although\\ntransformer-based models are known to only be effective when large training\\ndatasets are available, we show how we can effectively regularise the model\\nduring training and leverage pretrained image models to be able to train on\\ncomparatively small datasets. We conduct thorough ablation studies, and achieve\\nstate-of-the-art results on multiple video classification benchmarks including\\nKinetics 400 and 600, Epic Kitchens, Something-Something v2 and Moments in\\nTime, outperforming prior methods based on deep 3D convolutional networks. To\\nfacilitate further research, we release code at\\nhttps://github.com/google-research/scenic/tree/main/scenic/projects/vivit\",\n          \"Multi-query attention (MQA), which only uses a single key-value head,\\ndrastically speeds up decoder inference. However, MQA can lead to quality\\ndegradation, and moreover it may not be desirable to train a separate model\\njust for faster inference. We (1) propose a recipe for uptraining existing\\nmulti-head language model checkpoints into models with MQA using 5% of original\\npre-training compute, and (2) introduce grouped-query attention (GQA), a\\ngeneralization of multi-query attention which uses an intermediate (more than\\none, less than number of query heads) number of key-value heads. We show that\\nuptrained GQA achieves quality close to multi-head attention with comparable\\nspeed to MQA.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 49,\n        \"samples\": [\n          \"http://arxiv.org/pdf/1412.7062\",\n          \"http://arxiv.org/pdf/2103.15691\",\n          \"http://arxiv.org/pdf/2305.13245\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"categories\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 28,\n        \"samples\": [\n          \"Accepted to CVPR15\",\n          \"ECCV 2020\",\n          \"to appear in CVPR (2015)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"journal_ref\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"ECCV 2018\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"primary_category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"cs.AI\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"published\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 47,\n        \"samples\": [\n          \"20161116\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"updated\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 48,\n        \"samples\": [\n          \"20170504\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"references\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n","\n","texts = df['chunk'].to_list()\n","texts_embeddings = embedding_model.encode(texts)\n","texts_embeddings.shape"],"metadata":{"id":"0b-HPo-iFigA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717539177424,"user_tz":-180,"elapsed":90444,"user":{"displayName":"Nazarii Tovstenko","userId":"04428992633330999802"}},"outputId":"0877861f-ad65-4bcd-f596-0af856b3393a"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4838, 768)"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["chroma_client = chromadb.Client()\n","\n","collection = chroma_client.get_or_create_collection(name=\"rag\")\n","collection.add(\n","    embeddings = texts_embeddings,\n","    documents=texts,\n","    ids= [str(i) for i in range(len(texts))]\n",")"],"metadata":{"id":"BgCWAeOn8C0t","executionInfo":{"status":"ok","timestamp":1717539184024,"user_tz":-180,"elapsed":6606,"user":{"displayName":"Nazarii Tovstenko","userId":"04428992633330999802"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def query_vector_db(query, n_results=3):\n","    results = collection.query(\n","        query_embeddings = embedding_model.encode(query).tolist(),\n","        n_results=n_results\n","    )\n","    return results['documents']\n","query = \"What is llama2\"\n","retrieved_results = query_vector_db(query)\n","print(retrieved_results[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bmAjkddx9U7h","executionInfo":{"status":"ok","timestamp":1717539184025,"user_tz":-180,"elapsed":12,"user":{"displayName":"Nazarii Tovstenko","userId":"04428992633330999802"}},"outputId":"b046e444-2f18-47d4-cc5c-4e3216646620"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["['our responsible release strategy can be found in Section 5.3.\\nTheremainderofthispaperdescribesourpretrainingmethodology(Section2),ﬁne-tuningmethodology\\n(Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related\\nwork (Section 6), and conclusions (Section 7).\\n‡https://ai.meta.com/resources/models-and-libraries/llama/\\n§We are delaying the release of the 34B model due to a lack of time to suﬃciently red team.\\n¶https://ai.meta.com/llama\\n‖https://github.com/facebookresearch/llama\\n4\\nFigure 4: Training of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc : This process begins with the pretraining ofL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle using publicly\\navailableonlinesources. Followingthis,wecreateaninitialversionof L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc throughtheapplication', 'Ricardo Lopez-Barquilla, Marc Shedroﬀ, Kelly Michelena, Allie Feinstein, Amit Sangani, Geeta\\nChauhan,ChesterHu,CharltonGholson,AnjaKomlenovic,EissaJamil,BrandonSpence,Azadeh\\nYazdan, Elisa Garcia Anzano, and Natascha Parks.\\n•ChrisMarra,ChayaNayak,JacquelinePan,GeorgeOrlin,EdwardDowling,EstebanArcaute,Philomena Lobo, Eleonora Presani, and Logan Kerr, who provided helpful product and technical organization support.\\n46\\n•Armand Joulin, Edouard Grave, Guillaume Lample, and Timothee Lacroix, members of the original\\nLlama team who helped get this work started.\\n•Drew Hamlin, Chantal Mora, and Aran Mun, who gave us some design input on the ﬁgures in the\\npaper.\\n•Vijai Mohan for the discussions about RLHF that inspired our Figure 20, and his contribution to the\\ninternal demo.\\n•Earlyreviewersofthispaper,whohelpedusimproveitsquality,includingMikeLewis,JoellePineau,\\nLaurens van der Maaten, Jason Weston, and Omer Levy.', 'diminish any capabilities they might have oﬀered for those use cases.\\nWhile we attempted to reasonably balance safety with helpfulness, in some instances, our safety tuning goes\\ntoo far. Users of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc may observe an overly cautious approach, with the model erring on the side\\nof declining certain requests or responding with too many safety details.\\nUsersofthepretrainedmodelsneedtobeparticularlycautious,andshouldtakeextrastepsintuningand\\ndeployment as described in our Responsible Use Guide.§§\\n5.3 Responsible Release Strategy\\nReleaseDetails. Wemake L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle availableforbothresearchandcommercialuseat https://ai.meta.\\ncom/resources/models-and-libraries/llama/ . Thosewhouse L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle mustcomplywiththetermsof\\nthe provided license and our Acceptable Use Policy , which prohibit any uses that would violate applicable\\npolicies, laws, rules, and regulations.']\n"]}]},{"cell_type":"code","source":["compute_dtype = getattr(torch, \"float16\")\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=False,\n",")\n","\n","# Load base model\n","model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map={\"\": 0}\n",")\n","model.config.use_cache = True\n","model.config.pretraining_tp = 1\n","\n","# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n","\n","# Create pipe\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=5000)"],"metadata":{"id":"OJXpOgBFuSrc","colab":{"base_uri":"https://localhost:8080/","height":117,"referenced_widgets":["92ec957ebad9445b83083da1b7f2f620","5665872fa18043cd95d3b5ec42a62684","29eaa13707c5489ebc874640f2d317f9","4fda2d263df84838806a2cd009606ef0","652923b238bb479ca5c8d0f7913ff18b","08ae28512da24f07acf8f1342bba3bf2","bba8757c73d641c89deaf2569055a7cc","c40c8fc8fd234bfb98482631c8090e04","f645e160bb56433fa78bde14acc4be6b","a807238e0fdd43748125ea980f7cc9d3","91cf7db385134d1bb8dd0e852eabb075"]},"outputId":"b5257185-fd30-41b7-bfa0-84a3825f48dc","executionInfo":{"status":"ok","timestamp":1717539256087,"user_tz":-180,"elapsed":72071,"user":{"displayName":"Nazarii Tovstenko","userId":"04428992633330999802"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92ec957ebad9445b83083da1b7f2f620"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n","pip install xformers.\n"]}]},{"cell_type":"code","source":["def get_rag_responce(question):\n","    context = query_vector_db(question)\n","    full_prompt = f'''\n","[INST]\n","### Instruction: Give answer for the question strictly based on the context provided.\n","\n","### Input: {question}\n","\n","### Context : {context}\n","[/INST]\n","    '''\n","    result = pipe(full_prompt)\n","    return result[0]['generated_text']\n","\n","\n","prompt = \"What is llama2\"\n","print(get_rag_responce(prompt))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Luhj-gzrBLOk","executionInfo":{"status":"ok","timestamp":1717539268797,"user_tz":-180,"elapsed":12717,"user":{"displayName":"Nazarii Tovstenko","userId":"04428992633330999802"}},"outputId":"88f03e98-0a58-4301-eb09-7e5b07dfbd71"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","[INST]\n","### Instruction: Give answer for the question strictly based on the context provided.\n","\n","### Input: What is llama2\n","\n","### Context : [['our responsible release strategy can be found in Section 5.3.\\nTheremainderofthispaperdescribesourpretrainingmethodology(Section2),ﬁne-tuningmethodology\\n(Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related\\nwork (Section 6), and conclusions (Section 7).\\n‡https://ai.meta.com/resources/models-and-libraries/llama/\\n§We are delaying the release of the 34B model due to a lack of time to suﬃciently red team.\\n¶https://ai.meta.com/llama\\n‖https://github.com/facebookresearch/llama\\n4\\nFigure 4: Training of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc : This process begins with the pretraining ofL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle using publicly\\navailableonlinesources. Followingthis,wecreateaninitialversionof L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc throughtheapplication', 'Ricardo Lopez-Barquilla, Marc Shedroﬀ, Kelly Michelena, Allie Feinstein, Amit Sangani, Geeta\\nChauhan,ChesterHu,CharltonGholson,AnjaKomlenovic,EissaJamil,BrandonSpence,Azadeh\\nYazdan, Elisa Garcia Anzano, and Natascha Parks.\\n•ChrisMarra,ChayaNayak,JacquelinePan,GeorgeOrlin,EdwardDowling,EstebanArcaute,Philomena Lobo, Eleonora Presani, and Logan Kerr, who provided helpful product and technical organization support.\\n46\\n•Armand Joulin, Edouard Grave, Guillaume Lample, and Timothee Lacroix, members of the original\\nLlama team who helped get this work started.\\n•Drew Hamlin, Chantal Mora, and Aran Mun, who gave us some design input on the ﬁgures in the\\npaper.\\n•Vijai Mohan for the discussions about RLHF that inspired our Figure 20, and his contribution to the\\ninternal demo.\\n•Earlyreviewersofthispaper,whohelpedusimproveitsquality,includingMikeLewis,JoellePineau,\\nLaurens van der Maaten, Jason Weston, and Omer Levy.', 'diminish any capabilities they might have oﬀered for those use cases.\\nWhile we attempted to reasonably balance safety with helpfulness, in some instances, our safety tuning goes\\ntoo far. Users of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc may observe an overly cautious approach, with the model erring on the side\\nof declining certain requests or responding with too many safety details.\\nUsersofthepretrainedmodelsneedtobeparticularlycautious,andshouldtakeextrastepsintuningand\\ndeployment as described in our Responsible Use Guide.§§\\n5.3 Responsible Release Strategy\\nReleaseDetails. Wemake L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle availableforbothresearchandcommercialuseat https://ai.meta.\\ncom/resources/models-and-libraries/llama/ . Thosewhouse L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle mustcomplywiththetermsof\\nthe provided license and our Acceptable Use Policy , which prohibit any uses that would violate applicable\\npolicies, laws, rules, and regulations.']]\n","[/INST]\n","     Based on the context provided, \"llama2\" refers to the Llama model, which is a pre-trained language model developed by Meta AI. The model is described in the paper \"Llama: A Pre-trained Language Model for Text-to-Text Transformation\" and is available for both research and commercial use at <https://ai.meta.com/resources/models-and-libraries/llama/>. The paper outlines the model's development and training process, as well as its approach to model safety and relevant related work.\n"]}]},{"cell_type":"code","source":["prompt = \"What is attention in relation to llama2\"\n","print(get_rag_responce(prompt))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SyRW1YOKMxgh","executionInfo":{"status":"ok","timestamp":1717539294709,"user_tz":-180,"elapsed":25918,"user":{"displayName":"Nazarii Tovstenko","userId":"04428992633330999802"}},"outputId":"af8e73a7-084d-48fc-c051-0b8bed9c7adf"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","[INST]\n","### Instruction: Give answer for the question strictly based on the context provided.\n","\n","### Input: What is attention in relation to llama2\n","\n","### Context : [['attention graph, a path from node vat position k\\ninli, to node uat position minlj, is a series of\\nedges that connect these two nodes. If we look\\nat the weight of each edge as the proportion of\\ninformation transferred between two nodes, we\\ncan compute how much of the information at v\\nis propagated to uthrough a particular path by\\nmultiplying the weights of all edges in that path.\\nSince there may be more than one path between\\ntwo nodes in the attention graph, to compute the\\ntotal amount of information propagated from vtou,\\nwe sum over all possible paths between these two\\nnodes. At the implementation level, to compute the\\nattentions from litolj, we recursively multiply the\\nattention weights matrices in all the layers below.\\n~A(li) =\\x1a\\nA(li)~A(li\\x001)ifi > j\\nA(li) ifi=j(1)\\nIn this equation, ~Ais attention rollout, Ais raw attention and the multiplication operation is a matrix\\nmultiplication. With this formulation, to compute\\ninput attention we set j= 0.\\nAttention ﬂow In graph theory, a ﬂow network\\nis a directed graph with a “capacity” associated', 'Attention (S)   Joint Space-Time       Attention (ST)Divided Space-Time      Attention (T+S)  Sparse Local Global        Attention (L+G)  Axial Attention        (T+W+H)', '(`;a)time\\n(p;t)= SM0\\n@q(`;a)\\n(p;t)pDh>\\n\\x01\\x14\\nk(`;a)\\n(0;0)n\\nk(`;a)\\n(p;t0)o\\nt0=1;:::;F\\x151\\nA:\\n(11)\\nThe encoding z0(`)time\\n(p;t)resulting from the application of\\nEq. 8 using temporal attention is then fed back for spatial\\nattention computation instead of being passed to the MLP. In\\nother words, new key/query/value vectors are obtained from\\nz0(`)time\\n(p;t)and spatial attention is then computed using Eq. 6.\\nFinally, the resulting vector z0(`)space\\n(p;t)is passed to the MLP\\nof Eq. 9 to compute the ﬁnal encoding z(`)\\n(p;t)of the patch at\\nblock`. For the model of divided attention, we learn distinct query/key/value matrices fW(`;a)\\nQtime;W(`;a)\\nKtime;W(`;a)\\nVtimeg\\nIs Space-Time Attention All You Need for Video Understanding?']]\n","[/INST]\n","     Based on the context provided, attention in relation to llama2 refers to a mechanism for computing the importance of different parts of an input sequence. In the context of the llama2 model, attention is used to compute the weighted sum of the input patches at each layer, where the weights are learned during training. The attention mechanism is based on a path in a graph, where the edges represent the flow of information between nodes. The attention weights are computed by multiplying the weights of all edges in a path, and the total amount of information propagated from one node to another is computed by summing over all possible paths.\n","\n","In the llama2 model, the attention flow is computed using a flow network, where each node represents a patch in the input sequence and the edges represent the flow of information between patches. The attention weights are computed using a matrix multiplication operation, where the attention weights matrix is recursively applied to the lower layers of the network. The input attention is computed by setting the value of j to 0, and the attention flow is computed by feeding back the resulting vector from the application of temporal attention to the spatial attention computation.\n","\n","The llama2 model uses a combination of spatial and temporal attention to compute the final encoding of a patch at a block level. The spatial attention is computed using a multi-layer perceptron (MLP) and the temporal attention is computed using a flow network. The final encoding is then passed through an MLP to compute the final output of the model.\n","\n","In summary, attention in llama2 is a mechanism for computing the importance of different parts of an input sequence, and it is based on a path in a graph that represents the flow of information between patches in the input sequence. The attention weights are computed using a matrix multiplication operation, and the attention flow is computed by feeding back the resulting vector from the application of temporal attention to the spatial attention computation.\n"]}]}]}